# SQLite Server: High-Performance Pub/Sub Engine

SQLite Server includes a zero-dependency, ultra-low latency Publish/Subscribe (Pub/Sub) message broker baked directly into the database engine. It allows clients to stream real-time updates directly from the database and allows the database to broadcast its own changes autonomously.

## Core Capabilities

1.  **Durable Subscriptions:** If a client disconnects, the broker remembers exactly which messages they missed and streams them instantly upon reconnection.
2.  **Native SQL Integration:** You can publish messages completely from within SQL using virtual tables or functions.
3.  **Autonomous Triggers:** You can attach SQLite `AFTER UPDATE` triggers to your business tables to broadcast changes to the world without writing a single line of backend application code.
4.  **Extreme Throughput:** The broker uses custom Go `sync.Pool` zero-allocation memory pooling and automatically batches high-frequency publishes into single disk transactions, handling tens of thousands of messages per second.

---

## Publishing Data via SQL

There are two primary ways to publish messages from within SQLite. Understanding the difference is critical for maximizing performance.

### 1. The Virtual Table (`INSERT INTO vpubsub`)

The `vpubsub` table is a custom Virtual Table module. It intercepts `INSERT` commands and redirects the data directly into the Pub/Sub broker.

**When to use:**
*   Connecting an ORM (Prisma, GORM, Django) that natively understands standard `INSERT` statements.
*   **Database Triggers:** (Most common use case).

**Pros:**
*   **Standard SQL Syntax:** Works out-of-the-box with any ORM or SQL client.
*   **Trigger Support:** Can be used inside `BEFORE/AFTER INSERT/UPDATE` triggers, enabling row-level reactivity.
*   **Transactional Integrity:** Respects SQLite transactions (Atomicity). If the transaction rolls back, nothing is published.
*   **Type Safety:** Uses standard SQLite column types.

**Cons:**
*   **Single-Row Overhead:** When used outside a transaction, it incurs CGO overhead for every row (calling from C into Go logic).
*   **Sequential processing:** SQLite inserts rows one-by-one, which is slower than set-based processing for massive datasets.

**Examples:**

*Publish a single message:*
```sql
INSERT INTO vpubsub (channel, payload) VALUES ('logs', 'Application started successfully');
```

*Set up an autonomous alert system via Triggers:*
```sql
-- Create a business table
CREATE TABLE users (id INTEGER PRIMARY KEY, email TEXT, status TEXT);

-- Create a trigger that automatically publishes a message whenever a user is banned
CREATE TRIGGER on_user_banned
AFTER UPDATE OF status ON users
WHEN NEW.status = 'banned'
BEGIN
  INSERT INTO vpubsub (channel, payload)
  VALUES ('security_alerts', json_object('event', 'user_banned', 'email', NEW.email));
END;
```

### Transaction Buffering & Performance

To maximize throughput when using the Virtual Table, always wrap multiple `INSERT` statements in a single transaction (`BEGIN` / `COMMIT`). 

SQLite Server implements the `VTabTransaction` interface, allowing high-performance batching:

1.  **xBegin:** When an explicit transaction or a large `INSERT ... SELECT` starts, the virtual table switches to **Buffering Mode**.
2.  **xInsert:** Messages are appended to a high-speed Go slice (`[]pubsub.MsgPayload`) instead of being published to the broker immediately.
3.  **Sync Limit (1,000 Rows):** To prevent unbounded memory growth, if the buffer reaches 1,000 messages, it flushes a batch to the broker immediately using the broker's optimized `Publish()` method, then resets the buffer.
4.  **xCommit:** The final buffer is flushed, ensuring zero data loss.
5.  **xRollback:** The memory buffer is discarded immediately, ensuring nothing is published.

*Optimized bulk insert via Virtual Table:*
```sql
BEGIN;
-- SQLite Server buffers these in memory automatically
INSERT INTO vpubsub (channel, payload) SELECT ...; 
COMMIT; -- Entire batch flushes to broker at once
```

### 2. The Aggregate Function (`publish_batch()`)

`publish_batch(channel, payload)` is a custom SQL aggregate function (like `SUM` or `COUNT`). It collects all rows generated by a `SELECT` statement in Go memory and commits them to the broker in one massive, single transaction.

**When to use:**
*   Background workers or sync scripts pushing thousands of rows into a channel simultaneously.
*   Wiping and rebuilding frontend state via thousands of records.

**Pros:**
*   **Maximum Performance:** Eliminates row-by-row CGO overhead. It is the fastest way to publish data from within SQL.
*   **Set-Based:** Perfect for data migration, history sync, or mass broadcasting from a query result.
*   **Direct Broker Access:** Bypasses virtual table overhead and uses the broker's bulk ingestion system directly.

**Cons:**
*   **Trigger Limitation:** Difficult to use inside `AFTER INSERT` triggers effectively (triggers are row-level, this is set-level).
*   **Non-Standard Syntax:** Uses a custom aggregate function which might not be recognized by strict ORMs.
*   **Read-as-Write:** It looks like a `SELECT` statement but has side-effects (publishing).

### Implementation Details: How it works
The `publish_batch` function is an aggregate function with state:
1.  **Step:** As SQLite iterates through the result set, Go collects the items in a thread-safe memory buffer.
2.  **Sync Limit (1,000 Rows):** To prevent unbounded memory growth during massive bulk publishes, if the buffer reaches 1,000 messages, it flushes the current batch to the broker immediately and continues.
3.  **Finalize:** Once the query finishes, the "Final" step flushes any remaining buffered messages to the broker.

**Examples:**

*Publish 10,000 analytics events at once:*
```sql
-- This scans the table and publishes all new metrics instantly in one batch
SELECT publish_batch('analytics_feed', json_object('metric', name, 'value', val))
FROM raw_metrics
WHERE processed = false;
```

---

## Technical Comparison: Virtual Table vs. Aggregate Function

| Feature | Virtual Table (`vpubsub`) | Aggregate (`publish_batch`) |
| :--- | :--- | :--- |
| **Primary Goal** | Compatibility & Ease of Use | RAW Throughput |
| **SQL Syntax** | `INSERT INTO vpubsub(...)` | `SELECT publish_batch(...)` |
| **Best For** | Triggers, ORMs, Single entries | Data sync, Mass broadcasts |
| **Transaction Aware** | Yes (via `VTabTransaction`) | Yes (Query completion) |
| **Performance** | High (with `BEGIN/COMMIT`) | Extreme (Set-based) |
| **Sync Limit** | 1,000 Rows (Auto-flush) | 1,000 Rows (Auto-flush) |
| **CGO Overhead** | Row-by-row (mitigated by buffering) | Single call (Batch) |

---


## Architecture Summary

When you publish a message:
1. The backend groups it via the **Batcher** to minimize internal lock contention.
2. The message is durably written to `messages` in standard SQLite WAL mode.
3. Upon commit, the **Signal Hub** immediately broadcasts the payload via memory channels directly to active gRPC streams, dropping the latency to near-zero by completely bypassing database reads for live clients.
4. If a client was offline, the streaming endpoint falls back to scanning the `messages` table (`catchUp` phase) until the client reaches the live memory broadcast feed.


## Connecting Clients (RPC / HTTP)

Clients connect to the Pub/Sub broker using Connect RPC over HTTP/1.1 or HTTP/2 streams.

*Note: The `db_name` parameter should match your designated tenant or physical database name.*

### JavaScript / TypeScript Client Example

Ensure you are using the Connect RPC client to subscribe to the live, durable stream.

```typescript
import { createPromiseClient, ConnectError, Code } from "@connectrpc/connect";
import { DbService } from "./gen/db/v1/db_connect";

const client = createPromiseClient(DbService, transport);

// 1. Publish messages remotely
await client.publishBatch({
  dbName: "production_db",
  items: [
    { channel: "system_logs", payload: "Service spinning up..." },
    { channel: "system_logs", payload: "Cache cleared" }
  ]
});

// 2. Subscribe to a Durable Live Feed (using for-await loop)
// Best for simple scripts or backend processing
for await (const msg of client.subscribe({
    dbName: "production_db",
    subscriptionName: "frontend-dashboard-ui", // Unique consumer ID
    channel: "security_alerts"
})) {
  console.log(`[Message ID: ${msg.messageId}] ${msg.payload}`);
  // Expected Output:
  // [Message ID: 1] {"event":"user_banned","email":"hacker@example.com"}
}

// 3. Alternative: Subscribe with callback and manual cancellation
// Best for UI components (React/Vue/Svelte) to handle lifecycle cleanup
const cancel = subscribeWithCallback(
  {
    dbName: "production_db",
    subscriptionName: "frontend-dashboard-ui",
    channel: "security_alerts"
  },
  (msg) => {
    console.log(`[Message ID: ${msg.messageId}] ${msg.payload}`);
  },
  (err) => {
    console.error("Subscription error:", err);
  }
);

// Stop listening and close the connection whenever needed
cancel();

/**
 * Implementation helper for the callback-style subscriber
 */
function subscribeWithCallback(params, onMessage, onError) {
  const controller = new AbortController();

  (async () => {
    try {
      // Connect RPC uses the signal to handle cancellation
      const stream = client.subscribe(params, { signal: controller.signal });

      for await (const msg of stream) {
        onMessage(msg);
      }
    } catch (err) {
      // Ignore errors caused by manual cancellation (AbortController)
      if (err instanceof ConnectError && err.code === Code.Canceled) {
        return;
      }
      if (onError) onError(err);
    }
  })();

  // Return the cancel function for the consumer
  return () => controller.abort();
}
```
